{
  "annotations": { "list": [] },
  "editable": true,
  "gnetId": null,
  "graphTooltip": 0,
  "id": null,
  "iteration": 1685499177770,
  "links": [],
  "panels": [
    {
      "datasource": { "type": "prometheus", "uid": "prometheus" },
      "fieldConfig": {
        "defaults": { "unit": "s", "decimals": 3 },
        "overrides": []
      },
      "gridPos": { "h": 6, "w": 12, "x": 0, "y": 0 },
      "id": 1,
      "options": {
        "legend": { "displayMode": "list", "placement": "bottom" },
        "tooltip": { "mode": "single" }
      },
      "targets": [
        {
          "expr": "rate(chat_inference_duration_seconds_sum[1m]) / rate(chat_inference_duration_seconds_count[1m])",
          "interval": "",
          "legendFormat": "Avg inference time (s)",
          "refId": "A"
        }
      ],
      "title": "Average Inference Time",
      "type": "timeseries"
    },
    {
      "datasource": { "type": "prometheus", "uid": "prometheus" },
      "fieldConfig": {
        "defaults": { "unit": "reqps", "decimals": 0 },
        "overrides": []
      },
      "gridPos": { "h": 6, "w": 12, "x": 12, "y": 0 },
      "id": 2,
      "options": {
        "legend": { "displayMode": "list", "placement": "bottom" },
        "tooltip": { "mode": "single" }
      },
      "targets": [
        {
          "expr": "rate(http_requests_total{handler!=\"/metrics\"}[1m])",
          "legendFormat": "Requests per second",
          "refId": "A"
        }
      ],
      "title": "Request Rate",
      "type": "timeseries"
    },
    {
      "datasource": { "type": "prometheus", "uid": "prometheus" },
      "fieldConfig": {
        "defaults": { "unit": "percent", "decimals": 2 },
        "overrides": []
      },
      "gridPos": { "h": 6, "w": 12, "x": 0, "y": 6 },
      "id": 3,
      "options": {
        "legend": { "displayMode": "list", "placement": "bottom" },
        "tooltip": { "mode": "single" }
      },
      "targets": [
        {
          "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) * 100",
          "legendFormat": "5xx Error Rate (%)",
          "refId": "A"
        }
      ],
      "title": "Error Rate (5xx)",
      "type": "timeseries"
    },
    {
      "datasource": { "type": "prometheus", "uid": "prometheus" },
      "fieldConfig": {
        "defaults": { "unit": "s", "decimals": 3 },
        "overrides": []
      },
      "gridPos": { "h": 6, "w": 12, "x": 12, "y": 6 },
      "id": 4,
      "options": {
        "legend": { "displayMode": "list", "placement": "bottom" },
        "tooltip": { "mode": "single" }
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{handler=\"/chat\"}[1m])) by (le))",
          "legendFormat": "P95 Latency",
          "refId": "A"
        }
      ],
      "title": "Latency P95 (/chat)",
      "type": "timeseries"
    }
  ],
  "schemaVersion": 36,
  "style": "dark",
  "tags": ["inference", "model", "performance"],
  "templating": { "list": [] },
  "time": { "from": "now-30m", "to": "now" },
  "timepicker": {},
  "timezone": "",
  "title": "Model Inference Monitoring",
  "version": 2
}